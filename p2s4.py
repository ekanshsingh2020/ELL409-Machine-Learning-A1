# -*- coding: utf-8 -*-
"""p2s4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11aasITQGfJ6VbChmpl_4R7Jq5kJ5IBTE
"""

import numpy as np 
import gzip
import matplotlib.pyplot as plt
#from matplotlib.pyplot import imshow, show

## Handle the necessary imports


## Function to read the data from the compressed files
def read_data():
    train_images = gzip.open('train-images-idx3-ubyte.gz','r')
    train_labels = gzip.open('train-labels-idx1-ubyte.gz','r')
    image_size = 28
    num_images = 60000
    train_images.read(16)
    buffer = train_images.read(image_size * image_size * num_images)
    data_train_image = np.frombuffer(buffer, dtype=np.uint8).astype(np.float32)
    data_train_image = data_train_image.reshape(num_images, image_size, image_size, 1)
    y = [] 
    train_labels.read(8)
    for i in range(num_images):   
        buf = train_labels.read(1)
        labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)
        y.append(labels[0])
    y = np.array(y)
    X = []
    for i in data_train_image:
        xi = np.asarray(i).squeeze()
        X.append(xi.flatten())
    X = np.array(X)
    return X, y

## Function to get subset of data with labels 2 and 9
def get_subset(X, y):
    indices = np.where((y == 2) | (y == 9))
    X_subset = X[indices]
    y_subset = y[indices]
    return X_subset, y_subset

## In case you need the whole data, use this:
X, y = read_data()

## In case you need the subset of data with classes 2 and 9 use this:
X_subset, y_subset = get_subset(X, y)

np.save("train_images_subset.npy", X_subset)
np.save("train_labels_subset.npy", y_subset)

np.save("train_images.npy", X)
np.save("train_labels.npy", y)

X,Y=read_data()
print("Input shape X: " +str(X_subset.shape),"Input shape Y: "+str(y_subset.shape))

#diaplaying random example from dataset
img_dim=int(np.sqrt(X_subset.shape[1]))
def display_example():
    ind = np.random.randint(X_subset.shape[0]) #random index
    img = X_subset[ind].reshape(img_dim,img_dim)
    print(img)
    print("Number is:", y_subset[ind])
    plt.imshow(img)
display_example()

def normalize_X(data):
    mean = np.mean(data, axis=1, keepdims=True)

    std = np.std(data, axis=1, keepdims=True)
    normalized_data = (data - mean)/std
    return normalized_data

X=normalize_X(X_subset)

Y=np.floor(y_subset/5)  #to represent 2 as 0 and 9 as 1; can be obtained by dividing values of y with the point midway between both classes
shuffler = np.random.permutation(X.shape[0]) #shuffling dataset
X = X[shuffler]
Y = Y[shuffler]

X_train=X[:int(X.shape[0]*0.8),:].T      #0.8 to obtain 80:20 train:test ratio
Y_train=Y.reshape(Y.shape[0],1)[:int(Y.shape[0]*0.8),:].T
X_test=X[int(X.shape[0]*0.8):,:].T
Y_test=Y.reshape(Y.shape[0],1)[int(Y.shape[0]*0.8):,:].T
print("Shape of X_train: "+str(X_train.shape))
print("Shape of Y_train: "+str(Y_train.shape))
print("Shape of X_test: "+str(X_test.shape))
print("Shape of y_test: "+str(Y_test.shape))

def sigmoid(z):
    s = 1/(1+np.exp(-z))
    return s

def initialize_zero_parameters(dim):

    w = np.zeros((dim,1))
    b = 0
    
    return w, b

def forward_and_backward_propagate_sgd(w, b, X, Y,m):  
    
    X_sgd=X.reshape(X.shape[0],m)
    # Forward propagation
    A = sigmoid(np.dot(w.T,X_sgd) + b)              # computing activation
    cost = np.sum(((- np.log(A))*Y + (-np.log(1-A))*(1-Y)))  # computing cost
  
    # Backward propagation

    dw = (np.dot(X_sgd,(A-Y).T))
    db = (np.sum(A-Y))
    cost = np.squeeze(cost)

    grads = {"dw": dw,
             "db": db}
    
    return grads, cost
def iteration_sgd(w, b, X, Y, num_iterations, learning_rate, m):
    costs = []
    
    for i in range(num_iterations):
      for j in range(X.shape[1]//m):

        
        
        # cost and gradient calculation
        grads, cost = forward_and_backward_propagate_sgd(w, b, X[:,j*m:(j+1)*m], Y[:,j*m:(j+1)*m],m)

        dw = grads["dw"]
        db = grads["db"]
        
        # updating parameters
 
        w = w - (learning_rate*dw)
        b = b - (learning_rate*db)

        
      #if i % 10 == 0:
      costs.append(cost)

    params = {"w": w,
              "b": b}
    
    grads = {"dw": dw,
             "db": db}
    
    return params, grads, costs

def prediction(w, b, X):
    m = X.shape[1]
    Y_prediction = np.zeros((1,m))
    w = w.reshape(X.shape[0], 1)
    
    # predicting wheter image is 2 or 9
    A = sigmoid(np.dot(w.T,X) + b)           
    Y_prediction = (A >= 0.5) * 1.0    
    return Y_prediction

def model_sgd(X_train, Y_train, X_test, Y_test, batch_size,num_iterations, learning_rate = 0.01):
    
    # initialize parameters with zeros 
    w, b = initialize_zero_parameters(X_train.shape[0])

    # Gradient descent
    parameters, grads, costs = iteration_sgd(w, b, X_train, Y_train, num_iterations, learning_rate, batch_size)
    
    # obtaining trained parameters
    w = parameters["w"]
    b = parameters["b"]
    
    #prediction of test and training set
    Y_prediction_test = prediction(w, b, X_test)
    Y_prediction_train = prediction(w, b, X_train)


    # Print train/test Errors
    print("train accuracy: {} %".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))
    print("test accuracy: {} %".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))

    
    d = {"costs": costs,
         "Y_prediction_test": Y_prediction_test, 
         "Y_prediction_train" : Y_prediction_train, 
         "w" : w, 
         "b" : b,
         "learning_rate" : learning_rate,
         "num_iterations": num_iterations}
    
    return d

"""Maximum accuracy attained in all the trained models (including varying batch sizes) was in the case of stochastic gradient descent"""

d_sgd = model_sgd(X_train, Y_train, X_test,Y_test, 1,15, learning_rate = 0.01)

print("trained w parameter for sgd:")
print(d_sgd["w"])
print("trained b parameter for sgd:")
print(d_sgd["b"])

pred=open("preds.txt","w+")
for i in range(d_sgd["Y_prediction_test"].shape[1]):
  print(str(d_sgd["Y_prediction_test"][0,i]),end="\n",file=pred)