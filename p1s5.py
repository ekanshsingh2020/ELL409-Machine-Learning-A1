# -*- coding: utf-8 -*-
"""p1s5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JpSVRYUNFTIhdIiUYJekviHAG4nzDK0T
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

dataset1 = pd.read_csv('train.csv')
X_train = dataset1.iloc[ : , :-1].values
y_train = dataset1.iloc[:,-1].values
dataset2 = pd.read_csv('test.csv')
X_test = dataset2.iloc[ : , :-1].values
y_test = dataset2.iloc[:,-1].values

print(X_train.shape)

print(y_train.shape)

print(X_test.shape)

print(y_test.shape)

"""**Standardization of Data**"""

# Standard Scaling
def get_mean(x):
    return np.sum(x,axis=0)/x.shape[0]

def get_std(x):
    return np.std(x,axis =0)

def get_standardized_data(x):
    x_mean = get_mean(x)
    x_dif = x-x_mean
    sigma = get_std(x)
    return x_dif

X_train_standardized = get_standardized_data(X_train)

X_test_standardized = get_standardized_data(X_test)

print(X_test)

print(X_test_standardized)

"""**Normalization of Data**"""

def get_min(X):
  return X.min(axis = 0)

def get_max(X):
  return X.max(axis = 0)

def get_normalized_data(X):
    X_std = (X - get_min(X_train)) / (get_max(X_train) - get_min(X_train))
    return X_std

X_train_normalized = get_normalized_data(X_train)

print(X_train)

print(X_train_normalized)

X_test_normalized = get_normalized_data(X_test)

print(X_test)

print(X_test_normalized)

"""**Stochastic Gradient Descent**"""

import random
def stochastic_gradient_descent(x,y,epochs,learning_rate):
    number_of_features = x.shape[1]
    number_of_samples = x.shape[0]
    norm_x = get_normalized_data(x)
    initial_bias = np.ones((number_of_samples,1))
    norm_x = np.hstack((np.ones((number_of_samples,1)), norm_x))
    w = np.zeros(shape = (number_of_features+1))
    cost_list = []
    epoch_list = []
    for epoch in range(epochs):
      random_index = random.randint(0,number_of_samples-1)
      x_sample = norm_x[random_index]
      y_predicted = np.matmul(x_sample.T,w)
      dif = y[random_index] - y_predicted
      cost = np.mean(dif**2)/2
      w = w + (learning_rate)*(x_sample * dif)
      cost_list.append(cost)
      epoch_list.append(epoch)
    return w,cost_list,epoch_list

w_SGD,cost_list_SGD,epoch_list_SGD = stochastic_gradient_descent(X_train,y_train,10000,0.1)

print(w_SGD)

plt.xlabel("epoch")
plt.ylabel("cost")
plt.plot(epoch_list_SGD,cost_list_SGD)

def check_test_SGD(w,test,y_test):
  norm_x = get_normalized_data(test)
  number_of_samples = test.shape[0]
  norm_x = np.hstack((np.ones((number_of_samples,1)), norm_x))
  y_predicted = np.matmul(norm_x,w)
  error = y_test - y_predicted
  error = error/(len(y_test))
  return error

error_SGD = check_test_SGD(w_SGD,X_test,y_test)

print(np.mean(error_SGD))
meanloss100=abs(np.mean(error_SGD))

"""Batch size = 10%"""

import random
import math
def mini_batch_stochastic_gradient_descent1(x,y,epochs,learning_rate):
    number_of_features = x.shape[1]
    number_of_samples = x.shape[0]
    norm_x = get_normalized_data(x)
    initial_bias = np.ones((number_of_samples,1))
    norm_x = np.hstack((np.ones((number_of_samples,1)), norm_x))
    w = np.zeros(shape = (number_of_features+1))
    cost_list = []
    epoch_list = []
    size1 = int(math.floor(number_of_samples*0.1))
    for epoch in range(epochs):
      random_index = random.randint(0,number_of_samples-1)
      idx = np.random.randint(number_of_samples-1, size=size1)
      x_sample = norm_x[idx,:]
      y_sample = y[idx]
      y_predicted = np.matmul(x_sample,w)
      dif = y_sample - y_predicted
      cost = np.mean(dif**2)/2
      w = w + (learning_rate)*(np.mean(x_sample.T*dif,axis = 1))
      cost_list.append(cost)
      epoch_list.append(epoch)
    return w,cost_list,epoch_list

w_mini_BGD1,cost_list_mini_BGD1,epoch_list_mini_BGD1 = mini_batch_stochastic_gradient_descent1(X_train,y_train,10000,0.1)

print(w_mini_BGD1)

plt.xlabel("epoch")
plt.ylabel("cost")
plt.plot(epoch_list_mini_BGD1,cost_list_mini_BGD1)

def check_test_mini_BGD1(w,test,y_test):
  norm_x = get_normalized_data(test)
  number_of_samples = test.shape[0]
  norm_x = np.hstack((np.ones((number_of_samples,1)), norm_x))
  y_predicted = np.matmul(norm_x,w)
  error = y_test - y_predicted
  error = error/(len(y_test))
  return error

error_SGD1 = check_test_mini_BGD1(w_mini_BGD1,X_test,y_test)

print(np.mean(error_SGD1))
meanloss10=abs(np.mean(error_SGD1))

"""Batch Size = 20%"""

import random
import math
def mini_batch_stochastic_gradient_descent2(x,y,epochs,learning_rate):
    number_of_features = x.shape[1]
    number_of_samples = x.shape[0]
    norm_x = get_normalized_data(x)
    initial_bias = np.ones((number_of_samples,1))
    norm_x = np.hstack((np.ones((number_of_samples,1)), norm_x))
    w = np.zeros(shape = (number_of_features+1))
    cost_list = []
    epoch_list = []
    size1 = int(math.floor(number_of_samples*0.2))
    for epoch in range(epochs):
      random_index = random.randint(0,number_of_samples-1)
      idx = np.random.randint(number_of_samples-1, size=size1)
      x_sample = norm_x[idx,:]
      y_sample = y[idx]
      y_predicted = np.matmul(x_sample,w)
      dif = y_sample - y_predicted
      cost = np.mean(dif**2)/2
      w = w + (learning_rate)*(np.mean(x_sample.T*dif,axis = 1))
      cost_list.append(cost)
      epoch_list.append(epoch)
    return w,cost_list,epoch_list

w_mini_BGD2,cost_list_mini_BGD2,epoch_list_mini_BGD2 = mini_batch_stochastic_gradient_descent2(X_train,y_train,10000,0.1)

print(w_mini_BGD2)

plt.xlabel("epoch")
plt.ylabel("cost")
plt.plot(epoch_list_mini_BGD2,cost_list_mini_BGD2)

def check_test_mini_BGD2(w,test,y_test):
  norm_x = get_normalized_data(test)
  number_of_samples = test.shape[0]
  norm_x = np.hstack((np.ones((number_of_samples,1)), norm_x))
  y_predicted = np.matmul(norm_x,w)
  error = y_test - y_predicted
  error = error/(len(y_test))
  return error

error_SGD2 = check_test_mini_BGD2(w_mini_BGD2,X_test,y_test)

print(np.mean(error_SGD2))
meanloss20=abs(np.mean(error_SGD2))

"""Batch Size = 30%

"""

import random
import math
def mini_batch_stochastic_gradient_descent3(x,y,epochs,learning_rate):
    number_of_features = x.shape[1]
    number_of_samples = x.shape[0]
    norm_x = get_normalized_data(x)
    initial_bias = np.ones((number_of_samples,1))
    norm_x = np.hstack((np.ones((number_of_samples,1)), norm_x))
    w = np.zeros(shape = (number_of_features+1))
    cost_list = []
    epoch_list = []
    size1 = int(math.floor(number_of_samples*0.3))
    for epoch in range(epochs):
      random_index = random.randint(0,number_of_samples-1)
      idx = np.random.randint(number_of_samples-1, size=size1)
      x_sample = norm_x[idx,:]
      y_sample = y[idx]
      y_predicted = np.matmul(x_sample,w)
      dif = y_sample - y_predicted
      cost = np.mean(dif**2)/2
      w = w + (learning_rate)*(np.mean(x_sample.T*dif,axis = 1))
      cost_list.append(cost)
      epoch_list.append(epoch)
    return w,cost_list,epoch_list

w_mini_BGD3,cost_list_mini_BGD3,epoch_list_mini_BGD3 = mini_batch_stochastic_gradient_descent3(X_train,y_train,10000,0.1)

print(w_mini_BGD3)

plt.xlabel("epoch")
plt.ylabel("cost")
plt.plot(epoch_list_mini_BGD3,cost_list_mini_BGD3)

def check_test_mini_BGD3(w,test,y_test):
  norm_x = get_normalized_data(test)
  number_of_samples = test.shape[0]
  norm_x = np.hstack((np.ones((number_of_samples,1)), norm_x))
  y_predicted = np.matmul(norm_x,w)
  error = y_test - y_predicted
  error = error/(len(y_test))
  return error

error_SGD3 = check_test_mini_BGD3(w_mini_BGD3,X_test,y_test)

print(np.mean(error_SGD3))
meanloss30=abs(np.mean(error_SGD3))

"""Batch Size = 40%"""

import random
import math
def mini_batch_stochastic_gradient_descent4(x,y,epochs,learning_rate):
    number_of_features = x.shape[1]
    number_of_samples = x.shape[0]
    norm_x = get_normalized_data(x)
    initial_bias = np.ones((number_of_samples,1))
    norm_x = np.hstack((np.ones((number_of_samples,1)), norm_x))
    w = np.zeros(shape = (number_of_features+1))
    cost_list = []
    epoch_list = []
    size1 = int(math.floor(number_of_samples*0.4))
    for epoch in range(epochs):
      random_index = random.randint(0,number_of_samples-1)
      idx = np.random.randint(number_of_samples-1, size=size1)
      x_sample = norm_x[idx,:]
      y_sample = y[idx]
      y_predicted = np.matmul(x_sample,w)
      dif = y_sample - y_predicted
      cost = np.mean(dif**2)/2
      w = w + (learning_rate)*(np.mean(x_sample.T*dif,axis = 1))
      cost_list.append(cost)
      epoch_list.append(epoch)
    return w,cost_list,epoch_list

w_mini_BGD4,cost_list_mini_BGD4,epoch_list_mini_BGD4 = mini_batch_stochastic_gradient_descent4(X_train,y_train,10000,0.1)

print(w_mini_BGD4)

plt.xlabel("epoch")
plt.ylabel("cost")
plt.plot(epoch_list_mini_BGD3,cost_list_mini_BGD4)

def check_test_mini_BGD4(w,test,y_test):
  norm_x = get_normalized_data(test)
  number_of_samples = test.shape[0]
  norm_x = np.hstack((np.ones((number_of_samples,1)), norm_x))
  y_predicted = np.matmul(norm_x,w)
  error = y_test - y_predicted
  error = error/(len(y_test))
  return error

error_SGD4 = check_test_mini_BGD4(w_mini_BGD4,X_test,y_test)

print(np.mean(error_SGD4))
meanloss40=abs(np.mean(error_SGD4))

"""Batch Size = 50%

"""

import random
import math
def mini_batch_stochastic_gradient_descent5(x,y,epochs,learning_rate):
    number_of_features = x.shape[1]
    number_of_samples = x.shape[0]
    norm_x = get_normalized_data(x)
    initial_bias = np.ones((number_of_samples,1))
    norm_x = np.hstack((np.ones((number_of_samples,1)), norm_x))
    w = np.zeros(shape = (number_of_features+1))
    cost_list = []
    epoch_list = []
    size1 = int(math.floor(number_of_samples*0.5))
    for epoch in range(epochs):
      random_index = random.randint(0,number_of_samples-1)
      idx = np.random.randint(number_of_samples-1, size=size1)
      x_sample = norm_x[idx,:]
      y_sample = y[idx]
      y_predicted = np.matmul(x_sample,w)
      dif = y_sample - y_predicted
      cost = np.mean(dif**2)/2
      w = w + (learning_rate)*(np.mean(x_sample.T*dif,axis = 1))
      cost_list.append(cost)
      epoch_list.append(epoch)
    return w,cost_list,epoch_list

w_mini_BGD5,cost_list_mini_BGD5,epoch_list_mini_BGD5 = mini_batch_stochastic_gradient_descent5(X_train,y_train,10000,0.1)

print(w_mini_BGD5)

plt.xlabel("epoch")
plt.ylabel("cost")
plt.plot(epoch_list_mini_BGD5,cost_list_mini_BGD5)

def check_test_mini_BGD5(w,test,y_test):
  norm_x = get_normalized_data(test)
  number_of_samples = test.shape[0]
  norm_x = np.hstack((np.ones((number_of_samples,1)), norm_x))
  y_predicted = np.matmul(norm_x,w)
  error = y_test - y_predicted
  error = error/(len(y_test))
  return error

error_SGD5 = check_test_mini_BGD5(w_mini_BGD5,X_test,y_test)

print(np.mean(error_SGD5))
meanloss50=abs(np.mean(error_SGD5))

"""Loss vs Batch Size Plot"""

batchsize=([10,20,30,40,50,100])
lossmagn=([meanloss10,meanloss20,meanloss30,meanloss40,meanloss50,meanloss100])
plt.scatter(batchsize,lossmagn,color="red")
plt.title("Batch size vs loss")
plt.xlabel("Batch Size")
plt.ylabel("Loss")
plt.show()